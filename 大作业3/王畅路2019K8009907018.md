# 技术报告

2019K8009907018 王畅路

本报告实现了基于transformer的机器翻译模型

## 实验目的

本次实验通过使用pytorch完成基本机器翻译的任务，掌握文本生成数据的获取，预处理，语料对齐，分词等内容，理解并且可以实现编码器架构。

## 实验内容

1.了解注意力机制的原理，了解常用的注意力机器的实现方式

2.了解transformer的原理，使用torch.nn.transformer api进行调用

3.了解机器翻译的流程。

## 实验原理

1.Encoder-Decoder结构

编解码器结构用于输入和输出都是序列的任务，使用编码器和解码器两部分组成。编码器对于输入序列编码，得到一个或者一组表示向量C,这个向量涵盖了输入序列的语义信息，解码器用表示向量C
初始化解码出对应的目标序列。而机器翻译正好就是这种序列任务。

2.Attention机制

注意力机制是一种广泛应用于Encoder-Decoder架构的方法。Attention使得模型在解码的不同时刻可以有侧重地关注输入序列的不同部分，从而提升机器翻译任务的表现。

## 实验过程

### 数据预处理

数据预处理的操作主要包括

- 分词
- 构建词典
- 对句子进行tokenize
- 构建dataset 以及 dataloader
- 划分训练集以及验证集

```python
        jieba.lcut('数据集')
        # find all the sentence length between 2 and 30
        data_map=[2<len(i)<max_len for i in self.cn_data]
        new_cn=[self.cn_data[i] for i in range(len(self.cn_data)) if data_map[i]]
        new_en=[self.en_data[i] for i in range(len(self.en_data)) if data_map[i]]
        self.cn_data=new_cn
        self.en_data=new_en
        for i in trange(len(self.cn_data)):
            cn_sentence=jieba.lcut(self.cn_data[i].strip())
            for word in cn_sentence:
                if word not in cn_word2index:
                    cn_word2index[word]=len(cn_word2index)
                    cn_count[word]=1
                cn_count[word]+=1
            self.cn_data[i]=cn_sentence
        # rerange the word2index according to dic and reduce to max_word_num select the most 
        cn_words=list(sorted(cn_word2index.keys(),key=lambda x:cn_count[x],reverse=True))[:max_word_num]
        cn_word2index={word:i for i,word in enumerate(cn_words)}
        for i in range(len(self.cn_data)):
            self.cn_data[i]=[cn_word2index.get(word,unk) for word in self.cn_data[i]]
        # switch the word in sentence to index if unk use unk
        for i in trange(len(self.en_data)):
            en_sentence=self.en_data[i].strip().lower()
            en_sentence=en_sentence.replace(' \"','\" ').replace('(','( ')
            for p in ['.',',','!','?',':','\"',')']:
                en_sentence=en_sentence.replace(p,' '+p)
            en_sentence=en_sentence.split(' ')
            for word in en_sentence:
                if word not in en_word2index:
                    en_word2index[word]=len(en_word2index)
                    en_count[word]=1
                en_count[word]+=1
            self.en_data[i]=en_sentence
        en_words=list(sorted(en_word2index.keys(),key=lambda x:en_count[x],reverse=True))[:max_word_num]
        en_word2index={word:i for i,word in enumerate(en_words)}
        for i in range(len(self.en_data)):
            self.en_data[i]=[bos]+[en_word2index.get(word,unk) for word in self.en_data[i]]+[eos]
        # separate the dataset into train and valid
        cn_train,cn_val,en_train,en_val=train_test_split(self.cn_data,self.en_data,test_size=0.2,random_state=42)
        self.train_data=DATASET(cn_train,en_train)
        self.val_data=DATASET(cn_val,en_val)
```

关键代码如上，主要就是构建词表，同时把句子tokenize。

选定的最大的句子长度为30，词表中最多有10000个词，选择出现频率最高的前10000个词记入到词表中。

### 模型构建

使用transformer模型，在这个代码文件中，定义了基本的模型结构以及各个需要使用的函数。transformer模型部分参考了https://pytorch.org/tutorials/beginner/translation_transformer.html

`generate_square_subsequent_mask`和`creat_mask`函数用来生成模型解码的时候需要的mask.

```python
def translate(model,dicts,src_sentence,show=False):
    cn_word2index,cn_index2word,en_word2index,en_index2word=dicts
    def greedy_decode(model, src, src_mask, max_len, start_symbol):
        src = src.to(device)
        src_mask = src_mask.to(device)
        memory = model.encode(src, src_mask)
        l=[]
        ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)
        for i in range(max_len-1):
            memory = memory.to(device)
            tgt_mask = (generate_square_subsequent_mask(ys.size(0))
                        .type(torch.bool)).to(device)
            out = model.decode(ys, memory, tgt_mask)
            out = out.transpose(0, 1)
            prob = model.generator(out[:, -1])
            _, next_word = torch.max(prob, dim=1)
            next_word = next_word.item()
            ys = torch.cat([ys,
                            torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
            l.append(ys)
            if next_word == eos:
                break
        if show:
            att=None
            for ys in l:
                tgt_mask = (generate_square_subsequent_mask(ys.size(0)-1)
                            .type(torch.bool)).to(device)
                if att is not None:
                    att=torch.cat([att,get_attention(model,ys[:-1,:],memory,tgt_mask)[0][-1:]])
                else:
                    att=get_attention(model,ys[:-1,:],memory,tgt_mask)[0][-1:]
            attentions=att
            t1=[cn_index2word.get(i.item(),3) for i in src[:]]
            t2=[en_index2word.get(i.item(),0) for i in ys[1:-1]]
            show_attention(attentions.detach().cpu().numpy().T[:,:-1],t1,t2)
        return ys
    model.eval()
    with torch.no_grad():
        is_str=False
        if isinstance(src_sentence,str):
            is_str=True
            src_sentence=process(cn_word2index,src_sentence)
        src = torch.tensor(src_sentence).view(-1, 1)
        num_tokens = src.shape[0]
        src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)
        tgt_tokens = greedy_decode(
            model,  src, src_mask, max_len=60, start_symbol=0).flatten()
        if not is_str:
            return list(tgt_tokens.cpu().numpy())
    s=" ".join([en_index2word.get(i,3) for i in (list(tgt_tokens.cpu().numpy()))])\
    .replace("<sos> ", "").replace(" <eos>", "")
    return s
```

translate函数定义了预测时的函数，预测出eos词元就退出。

```python
def get_attention(model,ys,memory,tgt_mask):
    x=model.transformer.decoder.layers[0](model.positional_encoding(model.tgt_tok_emb(ys)),memory,tgt_mask)
    x=model.transformer.decoder.layers[1](x,memory,tgt_mask)
    ll=model.transformer.decoder.layers[2]
    x=ll.norm1(x+ll._sa_block(x,None,None))
    w=ll.multihead_attn(x,memory,memory,need_weights=True)[1]
    return w
def show_attention(attentions,src,tgt):
    fig = plt.figure(figsize=(15,8))
    ax = fig.add_subplot(111)
    ax.matshow(attentions,cmap='bone')
    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))
    maxlen=len(max(tgt,key=lambda x:len(x)))+len(tgt)/10
    if maxlen<10:
        rotation=45
    if maxlen>20:
        rotation=90
    if 10<=maxlen<=20:
        rotation=4.5*maxlen
    ax.set_xticklabels(['']+tgt,fontproperties=font,rotation=rotation)
    ax.set_yticklabels(['']+src,fontproperties=font)
    plt.tight_layout(pad=5)
    plt.show()
```

最后是展示注意力的代码，如上。

### 模型训练

首先处理数据获得词典

```python
device=torch.device('cuda')

print("Processing data...")
train_data,val_data,*dicts=make_dataset()
cn_word2index,cn_index2word,en_word2index,en_index2word=dicts
print(f"cn_vocab_size: {len(cn_word2index)}")
print(f"en_vocab_size: {len(en_word2index)}")

batch_size=32
train_loader=torch.utils.data.DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,collate_fn=pad_batch)
val_loader=torch.utils.data.DataLoader(dataset=val_data,batch_size=batch_size,collate_fn=pad_batch)
```

然后定义模型的各个参数，以及损失函数

```python
SRC_VOCAB_SIZE = len(cn_word2index)
TGT_VOCAB_SIZE = len(en_word2index)
EMB_SIZE = 128
NHEAD = 8
FFN_HID_DIM = 128
BATCH_SIZE = 128
NUM_ENCODER_LAYERS = 3
NUM_DECODER_LAYERS = 3

transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,
                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)
transformer = transformer.to(device)
loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad)
optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001)
```

然后是训练的函数

```python
def train_epoch(model, optimizer):
    model.train()
    losses = 0
    for src, tgt in tqdm(train_loader):
        src = src.to(device)
        tgt = tgt.to(device)
        tgt_input = tgt[:-1, :]
        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)
        optimizer.zero_grad()
        tgt_out = tgt[1:, :]
        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
        loss.backward()
        optimizer.step()
        losses += loss.item()
    return losses / len(train_loader)
```

之后是验证的函数，这里使用了各个词的loss作为验证的标准

```python
def evaluate(model):
    model.eval()
    losses = 0
    with torch.no_grad():
        for src, tgt in tqdm(val_loader):
            src = src.to(device)
            tgt = tgt.to(device)
            tgt_input = tgt[:-1, :]
            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)
            logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)
            tgt_out = tgt[1:, :]
            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))
            losses += loss.item()
    return losses / len(train_loader)
```

执行训练，每次epoch算一下loss，如果loss有减少的话就保存模型。

## 实验结果和分析

实验结果，在验证集上的loss变化如下。

关于模型和参数，模型主要是使用了已有的api函数，主要参数为：

- 词表大小为10000
- 词的维度设置为128维，用我的个人笔记本电脑使用gpu可以勉强跑下来
- 多头注意力使用了默认的8
- 学习率设置为0.0001
- 由于算力原因，基本没有进行调参。

训练的过程输出如下。

![image-20220710214646256](/home/lulu/.config/Typora/typora-user-images/image-20220710214646256.png)

注意力的可视化主要是读取保存的模型和词表，之后就可以直接运行。

举例如下。

输入是“我很喜欢上宗老师的课程”

![image-20220710214924470](/home/lulu/.config/Typora/typora-user-images/image-20220710214924470.png)

颜色越亮代表注意力分数就越高，可以观察到，上图中love teacher等词都具有较高的注意力分数，属于是语义贴合。但是一些没有具体涵义的词。而且宗老师，课程这样的词语都没有翻译出来。

我又尝试了一些比较短的句子。

![image-20220710215050177](/home/lulu/.config/Typora/typora-user-images/image-20220710215050177.png)

效果看起来还可以。

## 心得体会

通过本次实验我学会了如何使用transformer进行机器翻译任务，同时对seq2seq任务有了较深的认识，但是直接调用api的方法还是导致我对transformer的内部原理不太清楚。后续还要继续学习。
