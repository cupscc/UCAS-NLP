# 技术报告

2019K8009907018 王畅路

本次报告利用公开的情感或情绪分析评测语料，对比分析统计学习方法和神经网络方法的性能差异，并进行错误分析。

## 实验目的

本次实验通过使用pytorch transformers库完成了基本的情感分析任务，掌握了文本分类的一般过程，理解了文本分类的统计学习分类方法和神经网络分类方法。

## 实验内容

1.使用bert模型预训练并进行微调

2.使用朴素贝叶斯分类器对文本进行分类

3.对比统计学习方法和神经网络方法性能差异

## 实验原理

1.BERT模型

BERT模型通过收集大规模的无标注文本的预料，之后进行基于内容的文本分类标注数据或者进行少量情感分析标注。

BERT的结构如下，BERT仅仅使用了Transformer的编码器。BERT的预训练包含了掩码语言模型和句子衔接预测。

![image-20220710221054926](/home/lulu/.config/Typora/typora-user-images/image-20220710221054926.png)

利用大规模语料，反复迭代掩码语言模型任务和句子衔接预测任务，优化模型参数，当模型收敛或达到最大迭代次数时，即得到最终的BERT。之后为了用于下游任务，会使用少量标记的数据对模型进行微调，使得模型能够应用于真实的场景。

2.朴素贝叶斯模型

利用先验概率求得句子归属于某个类的后验概率，原理较为简单

3.tf-idf模型

单词的`TF-IDF` 值可以描述一个单词对文档的重要性，`TF-IDF` 值越大，则越重要。

- TF  Term Frequency，即词频（单词出现的频率），也就是一个单词在文档中出现的

  次数，次数越多越重要。

  计算公式：`一个单词的词频TF = 单词出现的次数 / 文档中的总单词数`

- IDF Inverse Document Frequency，即逆向文档词频，是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，这个单词对该文档就越重要，就越能通过这个单词把该文档和其他文档区分开。

  计算公式：`一个单词的逆向文档频率 IDF = log(文档总数 / 该单词出现的文档数 + 1)`

## 实验过程

本次实验采用了hugging face 网站上的imdb数据集，数据集的情况如下。

![image-20220710222422051](/home/lulu/.config/Typora/typora-user-images/image-20220710222422051.png)

### BERT模型

代码部分借鉴https://huggingface.co/docs/transformers/v4.20.1/en/training#compile-and-fit

数据预处理

tokenizer会使用hugging face的大量语料库对模型进行预训练，即'bert-base-uncased'

```python
import torch
from datasets import load_dataset
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
def tokenizer_func(examples):
    return tokenizer(examples['text'],padding='max_length',truncation=True)
tokenized_datasets = data_set.map(tokenizer_func,batched=True)
```

之后需要构建dataset dataloader,同时设置小批量的数据对模型进行微调

```python
from torch.utils.data import DataLoader
tokenized_datasets = tokenized_datasets.remove_columns(['text'])
tokenized_datasets = tokenized_datasets.rename_column("label","labels")
tokenized_datasets.set_format('torch')
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=1)
```

之后定义优化函数，定义学习率的变化趋势，训练轮数，由于算力限制，这里只训练三轮

```python
import torch
from transformers import BertForSequenceClassification
from transformers import get_scheduler
from torch.optim import AdamW

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
optimizer = AdamW(model.parameters(), lr=5e-5)
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
```

最后进行训练

```python
from tqdm.auto import tqdm
model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
```

得到模型之后就在验证集上进行验证

```python
output = []
model.eval()
for batch in eval_dataloader :
  batch = {k: v.to(device) for k, v in batch.items()}
  output.append(torch.argmax(model(**batch)[1],axis=-1).item())

from sklearn import metrics
score = metrics.accuracy_score(small_eval_dataset['labels'],output)
```

![image-20220710223340522](/home/lulu/.config/Typora/typora-user-images/image-20220710223340522.png)

可以看到最后的正确率是0.881

### NBM模型

要对文本进行分类，首先要做的是如何提取文本的主要信息

一篇文档是由若干词汇组成的，也就是文档的主要信息是词汇，可以用一些关键词来描述文档。

这种处理文本的方法叫做词袋(bag of words)模型，该模型会忽略文本中的词语出现的顺序以及相应的语法，将文档看做是由若干单词组成的，且单词之间互相独立，没有关联。

数据预处理部分和上一模型相同，不同的是，需要对数据进行特征提取。

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import warnings
warnings.filterwarnings('ignore')
tf = TfidfVectorizer(stop_words="english",max_df=0.5)
train_features = tf.fit_transform(train_text)
test_features = tf.transform(valid_text)
```

注意其中的stop_words,停用词是一些非常普遍使用的词语，对文档分析作用不大，在文档分析之前需要将这些词去掉。sklearn提供的TfidfVectorize函数提供了常用的英文停用词。

之后使用多项式贝叶斯分类

```python
clf = MultinomialNB(alpha=0.001).fit(train_features,train_label)
predicted_labels = clf.predict(test_features)
```

最后计算正确率如下

```python
from sklearn import metrics
score = metrics.accuracy_score(valid_label,predicted_labels)
score
```

![image-20220710225553595](/home/lulu/.config/Typora/typora-user-images/image-20220710225553595.png)

可以看到结果为0.758 比 BERT模型要差一些。

### 方法差异

在训练时间上，统计学习方法要远远快于深度学习方法。

但是在结果上，深度学习方法要好与统计学习方法。

从得到的结果上来说，神经网络方法得到的结果依旧有着难以解释的缺点。

我在Hugging Face官网上找到的大部分模型都是基于深度学习方法来进行的。

我找到一篇文献介绍了深度学习和机器学习对土耳其语分类的效果，效果如下。

![image-20220710231201536](/home/lulu/.config/Typora/typora-user-images/image-20220710231201536.png)

可以看到，无论从哪个方面，深度学习方法都是要强于统计学习方法的。然而，仅仅一种模型也是有上限的，它总规有自己的性能极限，最好的办法是融合几个模型以期得到更好的效果。文章通过整理以往的文献得到以下结果。

![image-20220710231433574](/home/lulu/.config/Typora/typora-user-images/image-20220710231433574.png)

可以看到，Doc2Vec+CNN的模型是最好的。

## 总结

本次由于上一个机器翻译实验占用了较多的时间，导致本实验所剩时间较少，完成的较为匆促，调研也较少。

主要是因为我低估了BERT模型的运行时间。本次实验仅仅阅读调研了一篇文献，有待加强。



文中提到文章为

Dogru, Hasibe & Hameed, Alaa & Tilki, Sahra & Jamil, Akhtar. (2021). Comparative Analysis of Deep Learning and Traditional Machine Learning Models for Turkish Text Classification. 