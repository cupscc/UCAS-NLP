# 技术报告

2019K8009907018 王畅路

## 问题描述

1. 给定语料库，给出其中部分词的词向量
2. 随机选择20个单词，计算和其词向量最相似的前十个单词
3. 对于同一批词汇，分别使用FNN，RNN,LSTM实现对词向量的计算

## data clean

使用北京大学标注语料。

在使用语料库的数据之前，需要对语料做如下处理

- 因为本次实验不关注词性，词性标注会对同一词的词向量产生影响，提取所有的词，去除词性标注
- 由于算力限制。选取2000个词进行处理，剩下的词记为UNK
- 去除标点符号和多余的空格和换行

```python
import  re
import  zhon.hanzi as zh
word_set = set()
with open("corpus.txt",'r',encoding='gbk')as f:
    text = f.read()
    rule_text = re.compile(r".*?(?=[\n])")
    sentences = re.findall(rule_text,text)
    rule_sentences = re.compile(r"[^\s].*?(?=[\s\s|\s\s\n])")
    with open("new_corpus.txt",'w',encoding='gbk') as fout:
        for sentence in sentences:
            tokens = re.findall(rule_sentences,sentence)
            for token in tokens:
                if token == ('' or ' '):
                    pass
                elif '/' in token:
                    index = token.find('/')#去除所有的词性标记
                    if(len(token) > 10):
                        continue
                    newstring = token[0:index]
                    if newstring in zh.punctuation: #去除所有的标点符号
                        continue
                    if(len(word_set) < 2000 and newstring not in word_set):#仅选取出现的前2000个词
                        word_set.add(newstring)
                    elif(len(word_set) >= 2000 and newstring not in word_set):
                        newstring = 'UNK'#超出的词记为UNK
                    fout.write(newstring+" ")
                else:
                    assert ("error")
            fout.write("\n")
```

处理前后的结果如下

处理前：

> 19980101-01-001-001/m  迈向/v  充满/v  希望/n  的/u  新/a  世纪/n  ——/w  一九九八年/t  新年/t  讲话/n  （/w  附/v  图片/n  １/m  张/q  ）/w  
> 19980101-01-001-002/m  中共中央/nt  总书记/n  、/w  国家/n  主席/n  江/nr  泽民/nr  
> 19980101-01-001-003/m  （/w  一九九七年/t  十二月/t  三十一日/t  ）/w  

处理后

> 迈向 充满 希望 的 新 世纪 —— 一九九八年 新年 讲话 附 图片 １ 张 
>
> 中共中央 总书记 国家 主席 江 泽民 
>
> 一九九七年 十二月 三十一日 

## 获得词向量

### 使用gensim

gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。它支持包括TF-IDF，LSA，LDA和word2vec在内的多种主题模型算法。这里我们使用gensim的word2vec算法进行实现。

首先导入gensim中处理文本的linesentence库和算法库word2vec

```python
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
```

我们选择文本的打开方式为gbk,不知为何 使用utf-8打开时总会出现编码错误

```python
with open('new_corpus.txt','r',encoding='gbk')as f:
    sentences = LineSentence(f)
    model = Word2Vec(sentences,vector_size = 10,window=5,min_count=1,max_vocab_size = 2000,workers=4,epochs= 1000)
    model.save('renmin.mdl')
```

Word2Vec函数中参数规定

- vector_size 为向量维度 为了节省算力 定义为10
- window 为窗口大小，也就是上下文大小
- min_count 文本中出现次数少于一次的文本忽略不计
- max_vocab_size 词向量表中的词的个数
- workers   线程数
- epoch  迭代次数

得到的词向量存储在模型中

使用TSNE把高维的词向量压缩到二维进行可视化

```python
tsne = TSNE(n_components=2, init='pca', n_iter=5000)
```

最后得到的词向量分布为

<img src="E:\2022spring\nlp\word2vect\word.png" alt="word" style="zoom:67%;" />

可以看出来，其中一些常见的固定短语或者主题相近的词语分布较为接近

例如： `发展` 和 `经济`两词在一起，`很` `更` `就` 程度副词相隔很近



接下来看接近的词向量，使用如下代码查看和发展最相近的10个词，其中的数字代表相似程度

```python
items = model.wv.most_similar('发展')
for i, item in enumerate(items):
    print(i, item[0], item[1])
```

输出为

> 0 国民经济 0.9464704394340515
> 1 生产力 0.9018236994743347
> 2 经济 0.8984900116920471
> 3 长期 0.8836050033569336
> 4 地位 0.88181471824646
> 5 推动 0.872458815574646
> 6 态势 0.8608289361000061
> 7 稳定 0.856752872467041
> 8 不断 0.8522592782974243
> 9 整个 0.8506758809089661

再查看和人民最相近的十个词

> 0 侨务 0.933482825756073
> 1 爱国 0.914237916469574
> 2 勤政廉政 0.8693817257881165
> 3 侨眷 0.8565348386764526
> 4 信赖 0.8555168509483337
> 5 宗旨 0.8551311492919922
> 6 归侨 0.8535509705543518
> 7 共产党 0.8296804428100586
> 8 团结 0.8248533010482788
> 9 各族 0.8182872533798218

作业要求选取十个词，此处仅列举两个，不再赘述。

### 使用pytorch搭建神经网络

首先我们需要知道的是，训练词向量有两种方式一种是CBOW方式，一种是skip-gram.

CBOW是使用上下文来预测当前词，而skip-gram是使用当前词来预测上下文。

<img src="https://img-blog.csdnimg.cn/img_convert/7a7ebbd1088eaf22cb6c3497b12cab56.png" alt="word2vec" style="zoom:67%;" />

治理我们使用skip-gram来训练词向量。

我们设置窗口的大小为2，也就是说我们再训练的时候，希望得到当前词前面两个词和后面两个词的概率分布。

接着我们需要做一些准备工作。

举个例子，现在有这样一段话   迈向 充满 希望 的 新 世纪

当当前词是`希望`的时候我们希望我们的训练集输入是这样的形式

> [希望，迈向]
>
> [希望，充满]
>
> [希望，的]
>
> [希望，新]
>
> [希望，世纪]

通过这样的驶入方式，我们可以让我们的模型最后输出的结果符合我们的预期，而文字是不能作为输入的，我们需要给每个文字附加上一个`id`通过这个`id`我们可以进行数学上的一些操作。

```python
with open("new_corpus.txt", "r", encoding="gbk") as f:
    lines = f.read().strip().split("\n")
    null_string = ''
    while(1):
        if(null_string in lines):
            lines.remove(null_string)
        else:
            break
    for line in lines :
        for word in line.split():
            word_set.add(word)
word_size = len(word_set)
word_to_id = {word:i for i,word in enumerate(word_set)}
id_to_word = {word_to_id[word]:word for word in word_to_id}
```

之后我们要创建类似上面提到的输入pair

```python
for line in lines:
    li_words = line.split()
    for i,word in enumerate(li_words):
        for j in range(-window_size,window_size+1):
            if i+j <0 or i+j >len(li_words) -1 or li_words[i+j] ==word:
                continue
            train_x.append(word_to_id[word])
            train_y.append(word_to_id[li_words[i+j]])
```

接下来我们来构建FNN网络，这里选择继承pytorch提供的Module库。

构造函数中，我们第一层网络使用nn提供的词嵌入方法设计。embedding的实现类似于字典，当我们提供输入的序号，embedding会根据这个需要把这个虚幻转换成一个向量。训练完成之后，embedding层就是训练得到的词向量。

第二层网络使用全连接的网络并进行softmax处理。

```python
class FNN(nn.Module):
    def __init__(self,word_size,window_size):
        super(FNN,self).__init__()
        self.embedding = nn.Embedding(word_size,10)
        self.linear = nn.Linear(10,word_size)
        self.log_softmax = nn.LogSoftmax()
```

前递函数只需要把输入一层一层向后传递即可。

```python
    def forward(self,x):
        x = self.embedding(x)
        x = self.linear(x)
        x = self.log_softmax(x)
        return x
```

训练时我们设置损失函数为NLLloss函数，训练五次，batch_size设置成10000

```
model = FNN(word_size,window_size).to(device)

loss_func = nn.NLLLoss()
lr = 1e-3
optimizer = torch.optim.Adam(model.parameters(),lr=lr)

model.train()
li_loss = []
batch_size = 10000
for epoch in range(0,5):
    for batch in range(0,len(train_x) - batch_size,batch_size):
        word = torch.tensor(train_x[batch:batch+batch_size]).long().to(device)
        label = torch.tensor(train_y[batch:batch + batch_size]).long().to(device)
        out = model(word)
        loss = loss_func(out,label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        print("hello",batch)
    li_loss.append(loss)
```

使用TSNE得到把词向量降维，我们得到如下的词向量分布

![fnn](E:\2022spring\nlp\word2vect\fnn.png)

可以看出得到的分布相比前面使用gensim框架得到的结果较为分散，聚类效果较差。

## 总结

通过本次作业我了解了很多关于词向量的知识，比如CBOW，skip-gram，word2vec方法等。同时我也了解了如何使用pytorch训练神经网络，如何搭建自己的神经网络，以及如何调整参数。

不过由于算力不足和本省知识不足，每次运行都要跑很久，因此只能通过降低迭代次数来减少训练时间，因此得到的结果并不好。

