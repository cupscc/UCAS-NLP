# NLP

## Week 1 简介

不同语言的独特问题：

- 屈折语
- 黏着语
- 孤立语

理性主义、经验主义、连结主义



### 习题

1-1.请说明如下句子有多少种不同的含义？
①He drew one card.、

他画了一张牌

他抽出一张牌

②咬死猎人的狗。

别人去咬死猎人的狗

狗把猎人咬死了

③鸡不吃了。

鸡不吃食物了

这顿饭不吃鸡了

1-2.试举例比较汉英句子的差异。

margin在英语中表示书的空白页的边缘，但是在汉语中却不这么说

中国有韭菜，西方却没有对应的英文

1-3.下列语言中哪些为自然语言？
世界语、C语言、鸟语、甲骨文

世界语 甲骨文

1-4.试列举不少于10种自然语言处理技术应用的场景。

扫描文本，处理文本，处理语音，语音转文字，方言转普通话，文字转语音。

1-5.通过对比测试Google翻译系统、微软Bing翻译系统和百度等翻译系统，了解机器翻译技术的性能现状。

## week2 数学

语言是稳态的可遍历性的随机过程

### 信息论

熵:是接受每条消息中包含的信息的平均量

$H(X) = - \sum_{x\in X} p(x)log_2p(x) $

> 熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大.

这里的消息代表来自分布或者数据流中的时间,样本或者特征.来自信源的另一个特征是样本的概率分布



通常熵的单位是二进制比特位. 熵越大 不确定性越大 要确定一个随机变量需要的二进制比特位越大 传递的信息越多



eg:

当每个概率相等的时候,下一个英文字母的不确定性更大.当有了一定规律的时候,下一个英文字母的不确定性更小了



白话文的熵比古文的熵更低,因此更容易预测,更简单,更不复杂(容易猜到下一个词是什么)



#### 联合熵

$H(X,Y) = -\sum_{x\in X}\sum_{y \in Y} p(x,y) log_2p(x,y)$ 

#### 条件熵

$H(X,Y) = H(X) +H(Y|X)$ 

#### 相对熵 relative entropy

也叫K-L距离 K-L散度

$D(p || q) = \sum_{x \in X}p(x) log \frac{p(x)}{q(x)}$  q是用来近似p的模型 相对熵经常用来衡量两个随机分布的差距.当两个随机分布相同的时候,相对熵是0.

#### 交叉熵 cross entropy

一个随机变量X~p(x), q(x) 为用于近似p(x)的概率分布,随机变量X和模型q之间的的交叉熵的定义是

$H(X,q) = H(X) + D(p||q)$

交叉熵也用来衡量估计模型q和真实概率分布之间的差异

> 相对熵和交叉熵的意义没什么区别,差了一个熵.一般用那个都行吧

对于语言领域来说,一个语言L=(X) ~ p(x)的交叉熵是

ppt

p是理论值 q是概率估计值

如果语言L是稳态的可遍历的随机过程.公式经过推到简化之后可以得到最常用的公式.

$H(L,q) = - \lim_{n->\infty} logq(x_1^n) $

一般都用这个公式来计算交叉熵 在设计模型的时候,交叉熵越小,一般模型越接近真实



交叉熵 = 熵 + 相对熵



#### 困惑度 perplexity

语言领域的交叉熵,语言L的困惑度记作 PP~q~ 

只不过是把交叉熵作为幂数取2次幂

#### 互信息 mutual information

如果(X,Y) ~ p(x,y),X,Y之间的互信息I(X;Y)定义为

$I(X;Y) = H(X) - H(X|Y)$ 

互信息是在知道了Y的值以后X的不确定性的减少量.也就是Y的值透露了多少X的信息量

#### 自信息

由于

$I(X;X) = H(X) -H(X|X) =H(X)$ ,所以熵也叫自信息.

一般来说 互信息的值越大,表示两个汉字越有可能组成一个词.



两个单个离散事件(不是变量)之间的互信息叫做点式互信息,有可能是负值.



### 应用

词汇歧义消除 word sense disambiguation WSD问题是自然语言处理中的基本问题之一.

词汇起义消除->上下文分类任务



## week 5 n模型

语料库

- 面向任务划分 
- 按照构成划分 同质语料库 异质语料库

N元语法

语言模型LM 只考虑当前词和历史基元构成的n元词序列 因此也叫n元语法模型 n元文法模型（注意是历史基元 不是未来基元）

- n = 1 
- n = 2 马尔科夫链 bi-gram
- n = 3 

标点符号也算 专有名词拆开 汉语可以分词 先变成基元



参数估计使用最大似然估计

分子是n元词组历史确定的时候出现wi的个数

分母是n元词组历史确定的个数



数据匮乏 数据稀疏->数据平滑



加一法

加V是因为历史基元后面可能跟任何的词，0次也算 都要加1 因此整个词表V都要加

平滑以后所有概率都变了



减值法/折扣法



高频词少 出现频率越高 这样的词的数量比较少



## week 6 条件随机场

## week 7 神经网络

前馈神经网络 查找表的初始的向量值是随机的
